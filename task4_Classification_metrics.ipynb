{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO85LnaDQTun+hB06kb7lUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moksha-nagraj/Marvel_tasks_lv1/blob/main/task4_Classification_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Metrics**\n",
        "Classification metrics are evaluation measures used to assess the performance of a classification model.\n",
        "\n",
        "## 1. Confusion Matrix\n",
        "A confusion matrix is a table that summarizes the performance of a classification algorithm. It consists of four metrics:\n",
        "\n",
        "True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "\n",
        "![alt confusion_matrix](https://tse1.mm.bing.net/th?id=OIP.dsG-IR8Ze7zVhtAi_iBEIwHaEK&pid=Api&P=0&h=180)\n",
        "## 2. Accuracy\n",
        "It measures the proportion of correctly predicted instances (both true positives and true negatives) among all instances in the dataset.\n",
        "\n",
        "![alt accuracy](https://tse4.mm.bing.net/th?id=OIP.r7T--7UyXc_ai__GK7JRtwHaCE&pid=Api&P=0&h=180)\n",
        "## 3.Precision\n",
        "Precision is a critical metric used to assess the quality of positive predictions made by a classification model. It quantifies the proportion of true positive predictions (correctly predicted positive instances) among all instances predicted as positive, whether they are true positives or false positives.\n",
        "\n",
        "![alt precision](https://tse3.mm.bing.net/th?id=OIP.hyNBcz8of3WhCjDfWHIwGAHaBH&pid=Api&P=0&h=180)\n",
        "## 4.F1-Score\n",
        "It provides a balanced assessment of a modelâ€™s performance, especially when there is an imbalance between the classes being predicted.\n",
        "![alt F1-Score](https://tse4.mm.bing.net/th?id=OIP.V1pzmYotN8tqFn3B79qWXwHaCD&pid=Api&P=0&h=180)\n"
      ],
      "metadata": {
        "id": "9Vk4gi-ku11R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Classification Metrics"
      ],
      "metadata": {
        "id": "o1S36ZhmzW6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Libraries\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
        "\n",
        "# True labels\n",
        "y_true = [0, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "# Predicted labels\n",
        "y_pred = [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "# Precision\n",
        "precision = precision_score(y_true, y_pred)\n",
        "# F1-Score\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpFukAbXzk17",
        "outputId": "0f9aae3a-3a05-4afc-9dfb-8af01c78881a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[3 2]\n",
            " [1 4]]\n",
            "Accuracy: 0.7\n",
            "Precision: 0.6666666666666666\n",
            "F1-Score: 0.7272727272727272\n"
          ]
        }
      ]
    }
  ]
}